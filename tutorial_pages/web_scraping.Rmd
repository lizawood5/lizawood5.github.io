---
Title: "Web scraping"
Subtitle: "Some `rvest` basics"
---

https://npgsweb.ars-grin.gov/gringlobal/search

GRIN database
```{r}
library(rvest)
library(xml2)
library(stringr)
```


```{r}
url <- read_html("https://npgsweb.ars-grin.gov/gringlobal/accessiondetail?id=1022773")

content <- url %>% 
  html_nodes(xpath = "//*[@id='nav-tabContent']")

summary <- html_children(content)[[1]]
passport <- html_children(content)[[2]] 
taxonomy <- html_children(content)[[3]] 
other <- html_children(content)[[4]] 
pedigree <- html_children(content)[[5]] 
ipr <- html_children(content)[[6]] 
observation <- html_children(content)[[7]] 

```

```{r}
html_text(passport)
html_nodes(passport, "div")
patterns.remove = c("\\n|\\r|\\t")

summary.text <- trimws(str_replace_all(str_remove_all(html_text(summary), patterns.remove), "\\s{2,}", " ")) ; summary.text
passport.text <- trimws(str_replace_all(str_remove_all(html_text(passport), patterns.remove), "\\s{2,}", " ")) ; passport.text
taxonomy.text <- trimws(str_replace_all(str_remove_all(html_text(taxonomy), patterns.remove), "\\s{2,}", " ")) ; taxonomy.text
other.text <- trimws(str_replace_all(str_remove_all(html_text(other), patterns.remove), "\\s{2,}", " ")) ; other.text
pedigree.text <- trimws(str_replace_all(str_remove_all(html_text(pedigree), patterns.remove), "\\s{2,}", " ")) ; pedigree.text
ipr.text <- trimws(str_replace_all(str_remove_all(html_text(ipr), patterns.remove), "\\s{2,}", " ")) ; ipr.text
#observation.text <- trimws(str_replace_all(str_remove_all(html_text(observation), patterns.remove), "\\s{2,}", " ")) ; observation.text

```




```{r}

url <- read_html("https://npgsweb.ars-grin.gov/gringlobal/accessiondetail?id=1017359")
# //*[@id="nav-tabContent"]

content <- url %>% 
  html_nodes(xpath = "//*[@id='nav-tabContent']")
summary <- html_children(content)[[1]]
passport <- html_children(content)[[2]] 
html_text(passport)
html_nodes(passport, "div")

passport.tax <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[6]]), patterns.remove)),"\\s{2,}", " ")  ; passport.tax 
passport.topname <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[9]]), patterns.remove)),"\\s{2,}", " ")  ; passport.topname
passport.origin <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[12]]), patterns.remove)),"\\s{2,}", " ")  ; passport.origin
passport.maintained <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[15]]), patterns.remove)),"\\s{2,}", " ")  ; passport.maintained
passport.recieved <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[18]]), patterns.remove)),"\\s{2,}", " ")  ; passport.recieved
passport.form <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[21]]), patterns.remove)),"\\s{2,}", " ")  ; passport.form
passport.history <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[24]]), patterns.remove)),"\\s{2,}", " ")  ; passport.history
passport.collected <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[27]]), patterns.remove)),"\\s{2,}", " ")  ; passport.collected
passport.donated <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[30]]), patterns.remove)),"\\s{2,}", " ")  ; passport.donated 
# From 33 to 68 unhelpful
passport.name <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[68]]), patterns.remove)),"\\s{2,}", " ")  ; passport.name
passport.narrative <- str_replace_all(trimws(str_remove_all(html_text(html_nodes(passport, "div")[[82]]), patterns.remove)),"\\s{2,}", " ")  ; passport.narrative




html_children(html_nodes(passport, "div")[[1]])

id_div <-  html_children(html_children(html_nodes(main, "div")[[6]])[[3]])[[1]]
```




# Make up accession numbers -- seven digits

```{r}
base <- "https://npgsweb.ars-grin.gov/gringlobal/accessiondetail?id="
ids <- seq(1000000, 1100000)

for(i in ids){
  read_html(paste0(base, i))
}
```



pull down each page's data






library(rvest)
library(xml2)
library(data.table)
library(stringr)
library(dplyr)

# Identify the URL to scrape documents from
# In this case, all of the IDs were stored on the one html page, which is great
url <- read_html("https://trid.trb.org/Results?txtKeywords=&txtTitle=&txtSerial=&ddlSubject=&txtReportNum=&ddlTrisfile=&txtIndex=&specificTerms=&txtAgency=&txtAuthor=&ddlResultType=&chkFulltextOnly=0&recordLanguage=&subjectLogic=or&dateStart=&dateEnd=&rangeType=publisheddate&sortBy=&sortOrder=DESC&rpp=100")

# Identify the title, and tridid -- which is the unique number that can be used to generate the url where all of the metadata lives, and combine them into a dataframe
main <-  html_nodes((html_nodes((html_nodes(url, "body")), "div")), "main")
id_div <-  html_children(html_children(html_nodes(main, "div")[[6]])[[3]])[[1]]
tridid <- data.frame(strsplit(html_attr(id_div, "value"), "\\|"))
colnames(tridid) = "tridid"

# Example for getting metadata from one page ----
url = read_html("https://trid.trb.org/View/1737466")
patterns.remove = c("\\n|\\r|\\t")

# Could Zoom in closer but I don't know if that really helps
#info <- url %>% html_nodes('body') %>% 
#  xml_find_all("//ul[contains(@class, 'record-info')]") %>% 
#  html_nodes('li')%>% 
#  html_text() 

info <- url %>% html_nodes('body') %>% 
  xml_find_all("//ul[contains(@class, 'record-info')]") %>% 
  html_text()

info <- info %>% str_remove_all(patterns.remove); info

record_url <- info[1] %>% str_extract("(?<=Record URL:).*(?=Availability:)|(?<=Record URL:).*(?=Record URL:)|(?<=Record URL:).*(?=Publication Date:)") %>% trimws()
URL <- info[1] %>% str_extract("(?<=URL:).*(?=Authors:)") %>% trimws()
authors <- info[1] %>% str_extract("(?<=Authors:).*(?=Publication Date:)") %>% trimws() %>% str_replace_all("\\s{2,}", ";")  #trimws(which = "left") %>% trimws(which = "right") %>% str_split("\\s{8,}") # this splits them out.
pub_year <- info[1] %>% str_extract("(?<=Publication Date:).*") %>% trimws()
pagination <- info[3] %>% str_extract("(?<=Pagination:).*(?=Serial:)") %>% str_remove_all("p") %>% trimws()
issue_number <- info[3] %>% str_extract("(?<=Issue Number:).*(?=Publisher:)") %>% trimws()
publisher <- info[3] %>% str_extract("(?<=Publisher:).*(?=ISSN:)|(?<=Publisher:).*(?=Publication flags:)") %>% trimws()
sponsoring_org <- info[3] %>% str_extract("(?<=Sponsor Organizations:).*(?=Performing Organizations:)") %>% trimws() %>% str_replace_all("\\s{2,}", "\\s")
performing_org <- info[3] %>% str_extract("(?<=Performing Organizations:).*(?=Principal Investigators:)") %>% trimws() %>% str_replace_all("\\s{2,}", "\\s")
PI <- info[3] %>% str_extract("(?<=Principal Investigators:).*(?=Start Date:)") %>% trimws() %>% str_replace_all("\\s{2,}", ";")
start_date <- info[3] %>% str_extract("(?<=Start Date:).*(?=Expected Completion Date:)") %>% trimws() 
end_date <- info[3] %>% str_extract("(?<=Expected Completion Date:).*(?=Actual Completion Date:)") %>% trimws()
completion_date <- info[3] %>% str_extract("(?<=Actual Completion Date:).*(?=USDOT Program:)") %>% trimws()
ISSN <- info[3] %>% str_extract("(?<=ISSN:).*(?=Publication flags:)|(?<=ISSN:).*(?=Serial URL:)|(?<=ISSN:).*(?=EISSN:)") %>% trimws()
ISBN <- info[5] %>% str_extract("(?<=ISBN:).*(?=Report/Paper Numbers:)") %>% trimws()

body <- data.frame(
  record_url,
  URL,
  authors,
  pub_year,
  pagination,
  issue_number,
  publisher,
  sponsoring_org,
  performing_org,
  PI,
  start_date,
  end_date,
  completion_date,
  ISSN,
  ISBN
)

## Getting the info that is from the head as metadata
name_nodes <- url %>% html_node("head") %>% html_nodes("meta") %>%
  html_attr("name")

content_nodes <- url %>% html_node("head") %>% html_nodes("meta") %>%
  html_attr("content")

head <- data.frame(cbind(name_nodes, content_nodes))
head.wide <- pivot_wider(body, names_from = name_nodes, values_from = content_nodes)

metadf <- cbind(head, body.wide)



# Create a loop feeding the IDs into the base url and pulling out metadata about documents ----
base_url = "https://trid.trb.org/View/"
patterns.remove = c("\\n|\\r|\\t")

index.head <- data.table()
index.body <- data.table()

Sys.time()
# In practice, I ran these in three chunks that took about 1 hour each: 
# e.g. for(i in tridid[5001:10000,]){
for(i in tridid){
  url <- read_html(paste0(base_url, i))
  body_recordinfo <- url %>% html_nodes('body') %>% 
    xml_find_all("//ul[contains(@class, 'record-info')]") %>% 
    html_text()
  
  info <- body_recordinfo %>% str_remove_all(patterns.remove)
  
  record_url <- info[1] %>% str_extract("(?<=Record URL:).*(?=Availability:)|(?<=Record URL:).*(?=Record URL:)|(?<=Record URL:).*(?=Publication Date:)") %>% trimws()
  authors <- info[1] %>% str_extract("(?<=Authors:).*(?=Publication Date:)") %>% trimws() %>% str_replace_all("\\s{2,}", ";") 
  pub_year <- info[1] %>% str_extract("(?<=Publication Date:).*") %>% trimws()
  pagination <- info[3] %>% str_extract("(?<=Pagination:).*(?=Serial:)") %>% str_remove_all("p") %>% trimws()
  issue_number <- info[3] %>% str_extract("(?<=Issue Number:).*(?=Publisher:)") %>% trimws()
  publisher <- info[3] %>% str_extract("(?<=Publisher:).*(?=ISSN:)|(?<=Publisher:).*(?=Publication flags:)") %>% trimws()
  sponsoring_org <- info[3] %>% str_extract("(?<=Sponsor Organizations:).*(?=Performing Organizations:)") %>% trimws() %>% str_replace_all("\\s{2,}", "\\s")
  performing_org <- info[3] %>% str_extract("(?<=Performing Organizations:).*(?=Principal Investigators:)") %>% trimws() %>% str_replace_all("\\s{2,}", "\\s")
  PI <- info[3] %>% str_extract("(?<=Principal Investigators:).*(?=Start Date:)") %>% trimws() %>% str_replace_all("\\s{2,}", ";")
  start_date <- info[3] %>% str_extract("(?<=Start Date:).*(?=Expected Completion Date:)") %>% trimws() 
  end_date <- info[3] %>% str_extract("(?<=Expected Completion Date:).*(?=Actual Completion Date:)") %>% trimws()
  completion_date <- info[3] %>% str_extract("(?<=Actual Completion Date:).*(?=USDOT Program:)") %>% trimws()
  ISSN <- info[3] %>% str_extract("(?<=ISSN:).*(?=Publication flags:)|(?<=ISSN:).*(?=Serial URL:)|(?<=ISSN:).*(?=EISSN:)") %>% trimws()
  ISBN <- info[5] %>% str_extract("(?<=ISBN:).*(?=Report/Paper Numbers:)") %>% trimws()
  
  body <- data.table(
    i,
    record_url,
    authors,
    pub_year,
    pagination,
    issue_number,
    publisher,
    sponsoring_org,
    performing_org,
    PI,
    start_date,
    end_date,
    completion_date,
    ISSN,
    ISBN
  )
  index.body <- rbind(body, index.body)
  
  ## Getting the info that is from the head as metadata
  name_nodes <- url %>% html_node("head") %>% html_nodes("meta") %>%
    html_attr("name")
  content_nodes <- url %>% html_node("head") %>% html_nodes("meta") %>%
    html_attr("content")
  
  head <- data.table(cbind(i, name_nodes, content_nodes))
  index.head <- rbind(head, index.head)
}
Sys.time()

# For some reason it does not like to pivot in the loop
index.head.wide <- index.head %>% pivot_wider(names_from = name_nodes, values_from = content_nodes)

# So then merge, full_join will automatically merge by i
trid_index <- full_join(index.body, index.head.wide)



trid_index$publication_date <- ifelse(trid_index$publication_date == "NULL", NA, trid_index$publication_date)
trid_index$citation_publication_date <- ifelse(trid_index$citation_publication_date == "NULL", NA, trid_index$citation_publication_date)
trid_index$citation_author <- ifelse(trid_index$citation_author == "NULL", NA, trid_index$citation_author)

fwrite(trid_index, '/Users/lizawood/Box/truckee/database_index/trid_index_unedited.csv', row.names = F)



